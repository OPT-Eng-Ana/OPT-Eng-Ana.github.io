<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Think Twice Before Claiming Your Optimization Algorithm Outperformance - Review and Beyond | You R. Name </title> <meta name="author" content="You R. Name"> <meta name="description" content="In this blog, we revisit the convergence analysis of first-order algorithms in minimization and minimax optimization problems. Within the classical oracle model framework, we review the state-of-the-art upper and lower bound results in various settings, aiming to identify gaps in existing research. With the rapid development of applications like machine learning and operation research, we further identify some recent works that revised the classical settings of optimization algorithms study."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://opt-eng-ana.github.io//blog/2024/opt-summary/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Think Twice Before Claiming Your Optimization Algorithm Outperformance - Review and Beyond",
            "description": "In this blog, we revisit the convergence analysis of first-order algorithms in minimization and minimax optimization problems. Within the classical oracle model framework, we review the state-of-the-art upper and lower bound results in various settings, aiming to identify gaps in existing research. With the rapid development of applications like machine learning and operation research, we further identify some recent works that revised the classical settings of optimization algorithms study.",
            "published": "April 28, 2024",
            "authors": [
              
              {
                "author": "Anonymous",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">You</span> R. Name </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Think Twice Before Claiming Your Optimization Algorithm Outperformance - Review and Beyond</h1> <p>In this blog, we revisit the convergence analysis of first-order algorithms in minimization and minimax optimization problems. Within the classical oracle model framework, we review the state-of-the-art upper and lower bound results in various settings, aiming to identify gaps in existing research. With the rapid development of applications like machine learning and operation research, we further identify some recent works that revised the classical settings of optimization algorithms study.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#notations-and-terminology">Notations and Terminology</a> </div> <div> <a href="#framework-oracle-model">Framework Oracle Model</a> </div> <div> <a href="#summary-of-results">Summary of Results</a> </div> <div> <a href="#to-have-a-better-bound">To Have a Better Bound</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> <div> <a href="#"></a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>In this blog, we review the convergence rate study in first-order optimization literature. Optimization problems generally are formulated as follows:</p> \[\min_{x\in\mathcal{X}}\ f(x).\] <p>Regarding the <strong>problem structure</strong>, we specifically consider <em>minimization</em> problems above and <em>minimax</em> optimization problems:</p> \[\min_{x\in\mathcal{X}}\ \left[f(x)\triangleq \max_{y\in\mathcal{Y}}\ g(x,y)\right].\] <p>Also based on the <strong>stochasticity structure</strong>, we divide our discussion into three cases:</p> <ul> <li>Deterministic (General) Optimization,</li> </ul> \[\min_{x\in\mathcal{X}}\ f(x)\] <ul> <li>Finite-Sum Optimization</li> </ul> \[\min_{x\in\mathcal{X}}\ f(x)\triangleq\frac{1}{n}\sum_{i=1}^n f_i(x)\] <ul> <li>(Purely) Stochastic Optimization</li> </ul> \[\min_{x\in\mathcal{X}}\ f(x)\triangleq\mathbb{E}_{\xi\sim\mathcal{D}}[f(x;\xi)]\] <p>A subtle while important difference between finite-sum and stochastic optimization problems lies in the ability to access to the whole function $f(x)$, so generally the classical SVRG algorithm<d-cite key="johnson2013accelerating"></d-cite> is unable to be applied in the purely stochastic case.</p> <p>Following the <strong>function structure</strong> of the objective, we will divide the discussion into various cases, including strongly-convex, convex and nonconvex cases; also for minimax problems, the discussion will be more complicated based on the convexity of each component, we will specify the settings later.</p> <h3 id="literature">Literature</h3> <p>This notes aims to review SOTA first-order optimization algorithms convergence results. In fact there already appeared several great works for comprehensive review of optimization algorithm from different perspectives. Besides many well-known textbook and course materials like the one from Stephen Boyd<d-cite key="boyd2024text"></d-cite><d-cite key="boyd2024video"></d-cite>, maybe one of the most impressive works is the blog post by Ruder<d-cite key="ruder2016overview"></d-cite>, which received more than 10k citations according to Google Scholar, this post reviewed algorithm design of gradient descent (GD), stochastic gradient descent (SGD) and their variants, especially those commonly used in machine learning community like AdaGrad<d-cite key="duchi2011adaptive"></d-cite> and Adam<d-cite key="kingma2014adam"></d-cite>. There are also several monographs which reviewed optimization algorithm in various settings, e.g., <d-cite key="bubeck2015convex"></d-cite>, <d-cite key="bottou2018optimization"></d-cite>, <d-cite key="sun2019survey"></d-cite>, <d-cite key="dvurechensky2021first"></d-cite> and <d-cite key="garrigos2023handbook"></d-cite>; the page by Ju Sun<d-cite key="sun2021list"></d-cite> was a popular repository tracking research effort on nonconvex optimization<d-footnote>Which unfortunately discontinued the update since 2022.</d-footnote>. The review by Ruoyu Sun<d-cite key="sun2019optimization"></d-cite> further specified the survey of optimization algorithm study in the context of deep learning. A recent survey by Danilova et al.,<d-cite key="danilova2022recent"></d-cite> revisited algorithm design and complexity analysis specifically in nonconvex optimization, which to some extent is the most close one matching the desire of our blog post.</p> <hr> <h2 id="framework-oracle-complexity-model">Framework: Oracle Complexity Model</h2> <p>Now we formally recall the definition of complexity. Here we stick to the classical <strong>oracle complexity model</strong><d-cite key="nemirovskij1983problem"></d-cite>. As the figure below suggest, generally this framework consists of the following components:</p> <ul> <li> <em>Fucntion class</em> $\mathcal{F}$, e.g., convex Lipschitz continuous function, and (nonconvex) Lipschitz smooth function.</li> <li> <em>Oracle class</em> $\mathbb{O}$, e.g., zeroth-order oracle (function value), first-order oracle (function gradient or subdifferential).</li> <li> <em>Algorithm class</em> $\mathcal{A}$, e.g., a common algorithm class studied in optimization literature is the <em>linear-span algorithm</em> interacting with an oracle $\mathbb{O}$, which encounters many well-known algorithms. This algorithm class is characterized as that if we let $(x^t)_t$ be the sequence history queries generated by the algorithm, for the next iterate, we have</li> </ul> \[x^{t+1}\in\mathrm{Span}\left\{x^0,\cdots,x^t;\mathbb{O}(f,x^0),\cdots,\mathbb{O}(f,x^t)\right\}.\] <ul> <li> <em>Complexity measure</em> $\mathcal{M}$, e.g., optimality gap $f(x)-f(x^\star)$ where $x^\star$ is the global minimum, function stationarity $||\nabla f(x)||$.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-28-opt-summary/complexity_analysis-480.webp 480w,/assets/img/2024-04-28-opt-summary/complexity_analysis-800.webp 800w,/assets/img/2024-04-28-opt-summary/complexity_analysis-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2024-04-28-opt-summary/complexity_analysis.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Oracle Complexity Framework (adapted from Prof. Yangyang Xu's Slides<d-cite key="xu2019slides"></d-cite>) </div> <p>The efficiency of algorithms is quantified by the <em>oracle complexity</em>: for an algorithm $\mathtt{A}\in\mathcal{A}(\mathbb{O})$ interacting with an oracle $\mathbb{O}$, an instance $f\in\mathcal{F}$, and the corresponding measurement $\mathcal{M}$, we define</p> \[T_{\epsilon}(f,\mathtt{A})\triangleq\inf\left\{T\in\mathbb{N}~|~\mathcal{M}(x^T)\leq\epsilon\right\}\] <p>as the minimum number of oracle calls $\mathcal{A}$ makes to reach convergence. So given an algorithm $\mathtt{A}$, its upper complexity bound for solving one specific function class $\mathcal{F}$ is defined as</p> \[\mathrm{UB}_\epsilon(\mathcal{F};\mathtt{A}) \triangleq \underset{f\in\mathcal{F}}{\sup}\ T_{\epsilon}(f,\mathtt{A}),\] <p>One of the mainstreams of optimization study is trying to design algorithms with better upper complexity bounds, corresponding to decreasing $\mathrm{UB}_\epsilon(\mathcal{F};\cdot)$ with their own algorihms. On the other hand, another stream of study focuses on the performance limit in terms of the worst-case complexity, i.e., the lower complexity bound (LB) of a class of algorithm under certain settings, which can be written as:</p> \[\mathrm{LB}_\epsilon(\mathcal{F},\mathcal{A},\mathbb{O}) \triangleq \underset{\mathtt{A}\in{\mathcal{A}(\mathbb{O})}}{\inf}\ \underset{f\in\mathcal{F}}{\sup}\ T_{\epsilon}(f,\mathtt{A}),\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-28-opt-summary/upper_lower-480.webp 480w,/assets/img/2024-04-28-opt-summary/upper_lower-800.webp 800w,/assets/img/2024-04-28-opt-summary/upper_lower-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2024-04-28-opt-summary/upper_lower.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Illustration of Upper and Lower Complexity Bounds </div> <p>As the figure above suggests, the optimization community keep facilitating the understanding of algorithm complexity from both upper and lower complexity directions. An ultimate goal in optimization algorithm complexity study is to find the <em>optimal algorithm</em> $\mathtt{A}^\star$ in a given setting, which means its upper bound matches with the lower bound of the algorithm class under the function setting, i.e.,</p> \[\mathrm{UB}_\epsilon(\mathcal{F};\mathtt{A}^\star)\asymp\mathrm{LB}_\epsilon(\mathcal{F},\mathcal{A},\mathbb{O}).\] <p>In this notes, we will focus on <strong>first-order algorithms</strong> in various optimization problem settings, trying to summarize the state-of-the-art (SOTA) UB and LB results, aiming to identify the gaps in existing reseach, and develop new trends.</p> <hr> <h2 id="notations">Notations</h2> <p>Besides the structure above, optimization algorithm convergence literature often require some other regularity conditions like Lipschitz smoothness, Lipschitz continuity, unbiased gradient estimator, bounded variance. Here we assume readers are already familiar with such context, interested readers may refer to these nice handbooks <d-cite key="garrigos2023handbook"></d-cite> and <d-cite key="danilova2022recent"></d-cite> for detailed definitions.</p> <p>For convenience, we summarize some of the notations commonly used in tables below.</p> <ul> <li>SC / C / NC / WC: strongly convex, convex, nonconvex, weakly-convex.</li> <li>FS: finite-sum.</li> <li>Stoc: stochastic</li> <li>$L$-S: $L$-Lipschitz smooth.</li> <li>$L$-IS / AS<d-footnote>For clarification, $L$-IS means in finite-sum problems, each component function $f_i$ itself is $L$-smooth, for the definition of $L$-AS, please refer to the definition of "mean-squared smoothness" in <d-cite key="arjevani2023lower"></d-cite>.</d-footnote>: $L$-Lipschitz individual / averaged smoothness.</li> <li>PL: Polyak-Łojasiewicz Condition</li> <li>Optimality gap: the function value gap $f(x) - f^\star$.</li> <li>Stationarity: the function gradient norm $|| \nabla f(x) ||$.</li> <li>Near-stationarity<d-cite key="davis2018stochastic"></d-cite>: the gradient norm $|| \nabla f_\lambda(x) ||$, where $f_\lambda$ is the Moreau envelope of the original function $f$.</li> <li>Duality Gap (for minimax optimization): the primal-dual gap of a given point $(x’, y’)$, defined as $\mathrm{gap}<em>f(x’, y’)\triangleq \max</em>{y\in\mathcal{Y}}f(x’,y)-\min_{x\in\mathcal{X}}f(x,y’)$.</li> <li>Primal Stationarity (for minimax optimization): the primal function gradient norm $|| \nabla \Phi(x) ||$ where $\Phi(x)\triangleq\max_{y\in\mathcal{Y}}f(x,y)$ is the primal function. It is different from the function stationarity in terms of the original objective function $f$.</li> </ul> <hr> <h2 id="summary-of-results">Summary of Results</h2> <p>As mentioned above, we categorize the discussion based on the problem, stochasticity and function structure, for convenience of presentation, we divide the presentation into the following cases:</p> <p><strong>Minimization Problems</strong></p> <ol> <li>Deterministic optimization</li> <li>Finite-sum and stochastic optimization</li> </ol> <p>Also for <strong>Minimax Problems</strong>, based on the convexity combination of each component, we consider the following cases:</p> <ol> <li>SC-SC/SC-C/C-C deterministic minimax</li> <li>SC-SC/SC-C/C-C finite-sum and stochastic minimax optimization</li> <li>NC-SC/NC-C deterministic minimax optimization</li> <li>NC-SC/NC-C finite-sum and stochastic minimax optimization</li> </ol> <h3 id="case-1-1-deterministic-minimization">Case 1-1: Deterministic Minimization</h3> <table> <thead> <tr> <th>Problem Type</th> <th>Measure</th> <th>Lower Bound</th> <th>Upper Bound</th> <th>Reference (LB-UB)<d-footnote>Note that here possibly we may not choose the most original work which proposed the results, rather we may select the literature which we are more familiar with, also with a clearer presentation. Readers are encouraged to check the reference therein for the original works.</d-footnote> </th> </tr> </thead> <tbody> <tr> <td>$L$-Smooth Convex</td> <td>Optimality gap</td> <td>$\Omega \left( \sqrt{L \epsilon^{-1}} \right)$</td> <td>$\checkmark$</td> <td>[<d-cite key="nesterov2018lectures"></d-cite>, Theorem 2.1.7; Theorem 2.2.2]</td> </tr> <tr> <td>$L$-Smooth $\mu$-SC</td> <td>Optimality gap</td> <td>$\Omega \left( \sqrt{\kappa} \log \frac{1}{\epsilon} \right)$</td> <td>$\checkmark$</td> <td>[<d-cite key="nesterov2018lectures"></d-cite>, Theorem 2.1.13]</td> </tr> <tr> <td>NS $L$-Lip Cont. Convex</td> <td>Optimality gap</td> <td>$\Omega (L^2 \epsilon^{-2})$</td> <td>$\checkmark$</td> <td> <d-cite key="bubeck2015convex"></d-cite>, Theorem 3.8</td> </tr> <tr> <td>NS $L$-Lip Cont. $\mu$-SC</td> <td>Optimality gap</td> <td>$\Omega (L^2 (\mu \epsilon)^{-1})$</td> <td>$\checkmark$</td> <td>[<d-cite key="bubeck2015convex"></d-cite>, Theorem 3.8]</td> </tr> <tr> <td>$L$-Smooth Convex</td> <td>Stationarity</td> <td>$\Omega \left( \sqrt{\Delta L \epsilon^{-1}} \right)$</td> <td>$\checkmark$</td> <td>[<d-cite key="carmon2021lower"></d-cite>, Theorem 1 &amp; Appendix A.1]</td> </tr> <tr> <td>$L$-Smooth NC</td> <td>Stationarity</td> <td>$\Omega (\Delta L \epsilon^{-2})$</td> <td>$\checkmark$</td> <td>[<d-cite key="carmon2020lower"></d-cite>, Theorem 1]</td> </tr> <tr> <td>NS $L$-Lip Cont. $\rho$-WC</td> <td>Near-stationarity</td> <td>Unknown</td> <td>$\mathcal{O}(\epsilon^{-4})$</td> <td>[<d-cite key="davis2018stochastic"></d-cite>, Theorem 2.1 implied]</td> </tr> <tr> <td>$\mu$-PL</td> <td>Optimality gap</td> <td> </td> <td>$\mathcal{O}(\epsilon^{-4})$</td> <td>[<d-cite key="davis2018stochastic"></d-cite>, Theorem 2.1 implied]</td> </tr> </tbody> </table> <p><strong>Remark:</strong></p> <ol> <li>content</li> </ol> <h3 id="case-1-2-finite-sum-and-stochastic-optimization-double-check">Case 1-2: Finite-sum and Stochastic Optimization (double check)</h3> <table> <thead> <tr> <th>Problem Type</th> <th>Measure</th> <th>Lower Bound</th> <th>Upper Bound</th> <th>Reference (LB-UB)</th> </tr> </thead> <tbody> <tr> <td>FS $L$-IS $\mu$-SC</td> <td>Optimality gap</td> <td>$\Omega \left( n + \sqrt{\kappa n} \log \frac{1}{\epsilon} \right)$</td> <td>$\checkmark$ ($\times$)</td> <td>[<d-cite key="woodworth2016tight"></d-cite>, Theorem 8] [<d-cite key="defazio2016simple"></d-cite>, Cor 6], [<d-cite key="allen2018katyusha"></d-cite>, Cor 2.1]</td> </tr> <tr> <td>FS $L$-IS C</td> <td>Optimality gap</td> <td>$\Omega \left( n + D \sqrt{n L \epsilon^{-1}} \right)$</td> <td>$\checkmark$ ($\times$)</td> <td>[<d-cite key="woodworth2016tight"></d-cite>, Theorem 7] [<d-cite key="allen2018katyusha"></d-cite>, Cor 3.7]</td> </tr> <tr> <td>FS $L$-AS $\mu$-SC</td> <td>Optimality gap</td> <td>$\Omega \left( n + n^{\frac{3}{4}} \sqrt{\kappa} \log \frac{\Delta}{\epsilon} \right)$</td> <td>$\checkmark$</td> <td>[<d-cite key="xie2019general"></d-cite>, Theorem 3.5], [<d-cite key="allen2018katyusha"></d-cite>, KatyushaX, Sec 5]</td> </tr> <tr> <td>FS $L$-AS C</td> <td>Optimality gap</td> <td>$\Omega \left( n + n^{\frac{3}{4}} D \sqrt{L \epsilon^{-1}} \right)$</td> <td>$\checkmark$</td> <td>[<d-cite key="zhou2019lower"></d-cite>, Theorem 4.2], [<d-cite key="allen2018katyusha"></d-cite>, KatyushaX, Sec 5]</td> </tr> <tr> <td>FS $L$-IS NC</td> <td>Stationarity</td> <td>$\Omega \left( \Delta L \epsilon^{-2} \right)$</td> <td>$\times$</td> <td>[<d-cite key="zhou2019lower"></d-cite>, Theorem 4.7], [<d-cite key="wang2019spiderboost"></d-cite>, Thm 1]</td> </tr> <tr> <td>FS $L$-AS NC</td> <td>Stationarity</td> <td>$\Omega \left( \sqrt{n \Delta L \epsilon^{-2}} \right)$</td> <td>$\checkmark$</td> <td>[<d-cite key="zhou2019lower"></d-cite>, Theorem 4.5], [<d-cite key="fang2018spider"></d-cite>, Thm 2, 3]</td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>Stoc $L$-$S$ $\mu$-SC</td> <td>Stationarity</td> <td>$\Omega (\epsilon^{-1})$</td> <td>$\checkmark$</td> <td>[<d-cite key="rakhlin2012making"></d-cite>, Thm 2] (?), [<d-cite key="ghadimi2012optimal"></d-cite>, Prop 9]</td> </tr> <tr> <td>Stoc $L$-$S$ C</td> <td>Stationarity</td> <td>$\Omega (\epsilon^{-2})$</td> <td>$\checkmark$</td> <td>[<d-cite key="woodworth2018graph"></d-cite>, Thm 1] (?), [<d-cite key="lan2012optimal"></d-cite>, Thm 1], Modified SA</td> </tr> <tr> <td>Stoc NS $\mu$-SC</td> <td>Stationarity</td> <td>$\Omega (\epsilon^{-2})$</td> <td>$\checkmark$</td> <td>[<d-cite key="agarwal2009information"></d-cite>, Thm 2], [<d-cite key="nemirovski2009robust"></d-cite>, Sec 2.1]</td> </tr> <tr> <td>Stoc NS C</td> <td>Stationarity</td> <td>$\Omega (\epsilon^{-2})$</td> <td>$\checkmark$</td> <td>[<d-cite key="agarwal2009information"></d-cite>, Thm 1], [<d-cite key="nemirovski2009robust"></d-cite>, Sec 2.2]</td> </tr> <tr> <td>Stoc $L$-$S$ $\mu$-SC</td> <td>Stationarity</td> <td>$\Omega (\epsilon^{-2})$ (implied)</td> <td>$\checkmark$</td> <td>[<d-cite key="foster2019complexity"></d-cite>, Theorem 1], AC-SA$^2$: $\mathcal{O} \left( \sqrt{\frac{LD}{\epsilon}} \log k + \frac{\sigma_x^2}{\epsilon^2} \log^3 k \right)$</td> </tr> <tr> <td>Stoc $L$-$S$ C</td> <td>Stationarity</td> <td>$\Omega \left( \frac{LD}{\epsilon} + \frac{\sigma_x^2}{\epsilon^2} \log \frac{LD}{\epsilon} \right)$</td> <td>$\checkmark$</td> <td>[<d-cite key="foster2019complexity"></d-cite>, Theorem 1, Corollary 1], AC-SA$^2$: $\mathcal{O} \left( \sqrt{\frac{LD}{\epsilon}} \log k + \frac{\sigma_x^2}{\epsilon^2} \log^3 k \right)$</td> </tr> <tr> <td>Stoc L-SS NC</td> <td>Stationarity</td> <td>$\Omega \left( \Delta \sigma_x \epsilon^{-4} \right)$</td> <td>$\checkmark$</td> <td>[<d-cite key="arjevani2023lower"></d-cite>, Theorem 1], [<d-cite key="ghadimi2013stochastic"></d-cite>, Cor 2.2]</td> </tr> <tr> <td>Stoc L-AS NC</td> <td>Stationarity</td> <td>$\Omega \left( \Delta \sigma_x^2 + 3 \sigma_x \epsilon^{-2} \right)$</td> <td>$\checkmark$</td> <td>[<d-cite key="arjevani2023lower"></d-cite>, Theorem 2], [<d-cite key="fang2018spider"></d-cite>, Theorem 1]</td> </tr> <tr> <td>Stoc L-AS IC</td> <td>Stationarity</td> <td>Unknown</td> <td>Unknown</td> <td>[<d-cite key="wang2019spiderboost"></d-cite>, Theorem 5]</td> </tr> <tr> <td>NS $L$-Lip $\rho$-WC</td> <td>Near-stationarity</td> <td>Unknown</td> <td>$\mathcal{O} (\epsilon^{-4})$</td> <td>[<d-cite key="davis2018stochastic"></d-cite>, Thm 2.1]</td> </tr> </tbody> </table> <p><strong>Remark:</strong></p> <ol> <li>content</li> </ol> <h3 id="case-2-1-sc-scsc-cc-c-deterministic-minimax-optimization">Case 2-1: SC-SC/SC-C/C-C Deterministic Minimax Optimization</h3> <table> <thead> <tr> <th>Problem Type</th> <th>Measure</th> <th>Lower Bound</th> <th>Upper Bound</th> <th>Reference (LB-UB)</th> </tr> </thead> <tbody> <tr> <td>C-C, bilinear, NS</td> <td>Duality Gap</td> <td>$\Omega(L / \epsilon)$</td> <td>$\checkmark$</td> <td>[<d-cite key="ouyang2021lower"></d-cite>, Thm 9], [<d-cite key="chambolle2011first"></d-cite>, Thm 1]</td> </tr> <tr> <td>C-C, general</td> <td>Duality Gap</td> <td>$\Omega(L D_X D_Y / \epsilon)$</td> <td>$\checkmark$</td> <td>[<d-cite key="xie2020lower"></d-cite>, Thm 3], [<d-cite key="nemirovski2004prox"></d-cite>, Thm 4.1]</td> </tr> <tr> <td>SC-C, bilinear, NS</td> <td>Duality Gap</td> <td>$\Omega(\sqrt{\kappa_x} / \epsilon)$</td> <td>$\checkmark$ ($\times$)</td> <td>[<d-cite key="ouyang2021lower"></d-cite>, Thm 10], [<d-cite key="chambolle2011first"></d-cite>, Thm 2] $\mathcal{O}(\kappa_x^2 / \sqrt{\epsilon})$</td> </tr> <tr> <td>SC-C, general</td> <td>Duality Gap</td> <td>$\Omega(D_Y \sqrt{L \kappa_x} / \epsilon)$</td> <td>$\checkmark$ ($\times$)</td> <td>[<d-cite key="xie2020lower"></d-cite>, Thm 2 imply], [<d-cite key="yang2020catalyst"></d-cite>, Sec 3.2] $\tilde{\mathcal{O}}(D \sqrt{L \kappa_x} / \epsilon)$</td> </tr> <tr> <td>C-SC, bilinear</td> <td>Duality Gap</td> <td>?</td> <td>$\mathcal{O}(\log \frac{1}{\epsilon})$</td> <td>(UB $\neq$ SC-C ?)?, [<d-cite key="du2019linear"></d-cite>, Thm 3.1]</td> </tr> <tr> <td>C-SC, general</td> <td>Duality Gap</td> <td>$\Omega(L D_X / \sqrt{\mu_y \epsilon})$</td> <td>?</td> <td>[<d-cite key="xie2020lower"></d-cite>, Thm 2 imply]</td> </tr> <tr> <td>SC-SC, bilinear</td> <td>Duality Gap</td> <td>$\Omega(\sqrt{\kappa_x \kappa_y} \log \frac{1}{\epsilon})$</td> <td>$\checkmark$</td> <td>[<d-cite key="zhang2022lower"></d-cite>, Thm 3.5], [<d-cite key="chambolle2016ergodic"></d-cite>, Thm 5, PIFO]</td> </tr> <tr> <td>SC-SC, general</td> <td>Duality Gap</td> <td>$\Omega(\sqrt{\kappa_x \kappa_y} \log \frac{1}{\epsilon})$</td> <td>$\checkmark$ ($\times$)</td> <td>[<d-cite key="zhang2022lower"></d-cite>, Thm 4.5], [<d-cite key="wang2020improved"></d-cite>, Thm 3]</td> </tr> <tr> <td>SC-SC, general</td> <td>Duality Gap</td> <td>same</td> <td>$\checkmark$ ($\times$)</td> <td>[<d-cite key="liang2019interaction"></d-cite>, Thm 1, (last iterate)] $\mathcal{O}(\sqrt{\kappa} \log \frac{1}{\epsilon})$</td> </tr> <tr> <td>PL-PL</td> <td>Duality Gap</td> <td>same</td> <td>$\checkmark$ ($\times$)</td> <td>[<d-cite key="liang2019interaction"></d-cite>, Thm 1, (last iterate)] $\mathcal{O}(\sqrt{\kappa} \log \frac{1}{\epsilon})$</td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> </table> <p><strong>Remark:</strong></p> <ol> <li>content</li> <li>LB SC-SC bilinear: [<d-cite key="ibrahim2020linear"></d-cite>, Cor 4] and extension to SCLI framework<d-cite key="arjevani2016lower"></d-cite> </li> </ol> <h3 id="case-2-2-sc-scsc-cc-c-finite-sum-and-stochastic-minimax-optimization">Case 2-2: SC-SC/SC-C/C-C Finite-sum and Stochastic Minimax Optimization</h3> <table> <thead> <tr> <th>Problem Type</th> <th>Measure</th> <th>LB</th> <th>UB</th> <th>Reference-LB</th> <th>Reference-UB</th> </tr> </thead> <tbody> <tr> <td>C-C, FS</td> <td>Duality Gap</td> <td>$\Omega(n + L / \epsilon)$</td> <td>$\times$</td> <td>[<d-cite key="xie2020lower"></d-cite>, Theorem 3, PIFO]</td> <td>[<d-cite key="yazdandoost2023stochastic"></d-cite>, Cor 2.1] $\mathcal{O}(\sqrt{n}/\epsilon)$</td> </tr> <tr> <td>C-C, Stoc, SS</td> <td>Duality Gap</td> <td>$\Omega(\epsilon^{-2})$</td> <td>$\checkmark$</td> <td>(Stoc C SS min)</td> <td>[<d-cite key="juditsky2011solving"></d-cite>, Cor 1]</td> </tr> <tr> <td>C-C, Stoc, NS</td> <td>Duality Gap</td> <td>$\Omega(\epsilon^{-2})$</td> <td>$\checkmark$</td> <td>(Stoc C NS min)</td> <td>[<d-cite key="nemirovski2009robust"></d-cite>, Lemma 3.1]</td> </tr> <tr> <td>SC-C, FS</td> <td>Duality Gap</td> <td>$\Omega\left(n + \sqrt{n L / \epsilon}\right)$</td> <td>$\checkmark$ ($\times$)</td> <td>(FS C min)</td> <td>[<d-cite key="yang2020catalyst"></d-cite>, Sec 3.2] $\tilde{\mathcal{O}}(\sqrt{n L / \epsilon})$</td> </tr> <tr> <td>C-SC, FS bilinear</td> <td>Duality Gap</td> <td>?</td> <td>?</td> <td>?</td> <td>[<d-cite key="du2019linear"></d-cite>, Thm 4.1] $\mathcal{O}(n \log \frac{1}{\epsilon})$</td> </tr> <tr> <td>C-SC, FS general</td> <td>Duality Gap</td> <td>$\Omega(n + L / \sqrt{\mu_y \epsilon})$</td> <td>?</td> <td>[<d-cite key="xie2020lower"></d-cite>, Thm 2]</td> <td>(implies from SC-C?)</td> </tr> <tr> <td>SC-SC, FS</td> <td>Duality Gap</td> <td>$\Omega\left((n + \kappa) \log \frac{1}{\epsilon}\right)$</td> <td>$\checkmark$ ($\times$)</td> <td>[<d-cite key="xie2020lower"></d-cite>, Thm 1]</td> <td>[<d-cite key="palaniappan2016stochastic"></d-cite>, Thm 1] $\mathcal{O}((n + \kappa) \log \frac{1}{\epsilon})$</td> </tr> <tr> <td>SC-SC, Stoc, SS</td> <td>Duality Gap</td> <td>$\Omega(\epsilon^{-1})$</td> <td>$\checkmark$</td> <td>(Stoc SC SS min)</td> <td>[<d-cite key="hsieh2019convergence"></d-cite>, Thm 5, pt. dist.] $\mathcal{O}(\epsilon^{-1})$</td> </tr> <tr> <td>SC-SC, Stoc, NS</td> <td>Duality Gap</td> <td>$\Omega(\epsilon^{-1})$</td> <td>$\checkmark$</td> <td>(Stoc SC NS min)</td> <td>[<d-cite key="yan2020optimal"></d-cite>, Thm 1, in prob.] $\mathcal{O}(\epsilon^{-1})$</td> </tr> <tr> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> </tbody> </table> <p><strong>Remark:</strong></p> <ol> <li>content</li> <li>UB SC-SC general: [<d-cite key="luo2019stochastic"></d-cite>], [<d-cite key="chavdarova2019reducing"></d-cite>, (cocoercive?)]</li> </ol> <h3 id="case-2-3-nc-scnc-c-deterministic-minimax-optimization">Case 2-3: NC-SC/NC-C Deterministic Minimax Optimization</h3> <table> <thead> <tr> <th>Type</th> <th>Measure</th> <th>LB</th> <th>UB</th> <th>Reference-LB</th> <th>Reference-UB</th> </tr> </thead> <tbody> <tr> <td>NC-SC, Deter</td> <td>Primal Stationarity</td> <td>$\Omega(\Delta \mathcal{L} \epsilon^{-2})$</td> <td>$\checkmark$ ($\times$)</td> <td>[<d-cite key="zhang2021complexity"></d-cite>, Theorem 3.1]</td> <td>[<d-cite key="zhang2021complexity"></d-cite>, Theorem 4.1]</td> </tr> <tr> <td>NC-C, Deter</td> <td>Primal Stationarity</td> <td>$\Omega(\Delta \mathcal{L} \epsilon^{-2})$</td> <td>$\times$</td> <td> </td> <td>[<d-cite key="lin2020near"></d-cite>, Cor A.8] $\mathcal{O}(\Delta L^2 \epsilon^{-3} \log^2 \frac{1}{\epsilon})$</td> </tr> <tr> <td>WC-C, Deter</td> <td>Primal Stationarity</td> <td>?</td> <td>?</td> <td> </td> <td>[<d-cite key="boct2023alternating"></d-cite>, Thm 3.7] $\mathcal{O}(\epsilon^{-6})$ (single loop)</td> </tr> <tr> <td>NC-PL, Deter</td> <td>Primal Stationarity</td> <td>$\Omega(\sqrt{\kappa \Delta \mathcal{L} \epsilon^{-2}})$</td> <td>$\times$</td> <td> </td> <td>[<d-cite key="yang2022faster"></d-cite>, Thm 3.1]</td> </tr> <tr> <td>NC-SC, Deter</td> <td>Stationarity</td> <td>$\Omega(\sqrt{\kappa \Delta \mathcal{L} \epsilon^{-2}})$</td> <td>$\times$</td> <td> </td> <td>[<d-cite key="lin2020near"></d-cite><d-cite key="xu2023unified"></d-cite>, Thm 3.1] $\mathcal{O}(\epsilon^{-2})$</td> </tr> <tr> <td>NC-C, Deter</td> <td>Stationarity</td> <td>$\Omega(\Delta \mathcal{L} \epsilon^{-2})$</td> <td>$\times$</td> <td> </td> <td>[<d-cite key="lin2020near"></d-cite>, Cor 6.2] $\mathcal{O}(\sqrt{D \Delta \mathcal{L}^{1.5} \epsilon^{-2.5} \log^2 \frac{1}{\epsilon}})$ (NC S min)</td> </tr> <tr> <td>SC-NC, Deter</td> <td>Stationarity</td> <td>$\Omega(\sqrt{\kappa} \log \frac{1}{\epsilon})$</td> <td>$\times$</td> <td> </td> <td>[<d-cite key="xu2023unified"></d-cite>, Cor 4.1] $\mathcal{O}(\epsilon^{-2})$</td> </tr> <tr> <td>C-NC, Deter</td> <td>Stationarity</td> <td>$\Omega(\sqrt{L/\epsilon})$</td> <td>$\times$</td> <td> </td> <td>[<d-cite key="xu2023unified"></d-cite>, Cor 4.2] $\mathcal{O}(\epsilon^{-4})$</td> </tr> </tbody> </table> <p><strong>Remark:</strong></p> <ol> <li>content</li> <li>[ZXSL20, Thm 3.8] $\mathcal{O}(\epsilon^{-2})$ (unclear dependence)</li> </ol> <h3 id="case-2-4-nc-scnc-c-finite-sum-and-stochastic-minimax-optimization">Case 2-4: NC-SC/NC-C Finite-sum and Stochastic Minimax Optimization</h3> <table> <thead> <tr> <th>Type</th> <th>Measure</th> <th>LB</th> <th>UB</th> <th>Reference (UB &amp; LB)</th> </tr> </thead> <tbody> <tr> <td>NC-SC, FS, AS</td> <td> </td> <td>$\Omega\left(\frac{\sqrt{n\kappa}\Delta L}{\epsilon^2}\right)$</td> <td>✔(x)</td> <td>[XWLP20, Cor 1] $O\left(\sqrt{n}\kappa^2 L\Delta\epsilon^{-2}\right)$ [ZYG+21, Thm 3.2] [ZYG+21, Sec 4.2] $O\left(n^{3/4}\sqrt{\kappa}L\Delta\epsilon^{-2}\right)$</td> </tr> <tr> <td>NC-SC, FS, IS</td> <td> </td> <td>$\Omega\left(\frac{\sqrt{n}\Delta L}{\epsilon^2}\right)$</td> <td>✔(x)</td> <td>[XWLP20, Cor 1] $O\left(\sqrt{n}\kappa_{ky}L\Delta\epsilon^{-2}\right)$ (NC IS FS min)</td> </tr> <tr> <td>NC-C, FS, IS</td> <td> </td> <td>$\Omega\left(\frac{\sqrt{n}\Delta L}{\epsilon^2}\right)$</td> <td>✔(x)</td> <td>[DH19, Thm 2] $O\left(n\epsilon^{-2} + \epsilon^{-6}\right)$ (NC AS FS) (IS?)</td> </tr> <tr> <td>NC-C, FS, IS</td> <td> </td> <td>$\Omega\left(\frac{\sqrt{n}\Delta L}{\epsilon^2}\right)$</td> <td>✘(x)</td> <td>[YZKH20, Sec 4.2]: $O\left(n^{3/4}L^2D_y\Delta\epsilon^{-3}\right)$ (NC AS FS) (IS?)</td> </tr> <tr> <td>NC-SC, Stoc, SS</td> <td> </td> <td>$\Omega\left(\frac{\Delta L\sigma^2}{\epsilon^4}\right)$</td> <td>✘</td> <td>[YOLH21, Cor 4.1] (NC SS Stoc min)</td> </tr> <tr> <td>NC-SC, Stoc, AS</td> <td> </td> <td>$\Omega\left(\frac{\Delta L\sigma^2 + \sigma^2}{\epsilon^3}\right)$</td> <td>✔(x)</td> <td>[XWLP20, Thm 1] $O\left(\Delta L\sigma^2\kappa_{ky}^3\epsilon^{-3}\right)$ (NC AS Stoc min)</td> </tr> <tr> <td>NC-C, Stoc, SS</td> <td> </td> <td>$\Omega\left(\frac{\Delta L\sigma + \sigma^2}{\epsilon^3}\right)$</td> <td>✘</td> <td>[LJJ19, Thm 4.9] [BB20, Thm 3.13] $O\left(\epsilon^{-8}\right)$ (SL, Lip-C) (NC AS Stoc) (IS?)</td> </tr> <tr> <td>WC-SC, Stoc</td> <td> </td> <td>?</td> <td>?</td> <td>[YXL+20, Thm 2] $O\left(c^{-4}\right)$</td> </tr> <tr> <td>WC-C, Stoc</td> <td> </td> <td>?</td> <td>?</td> <td>[RLLY18, Thm 1] $O\left(\epsilon^{-6}\right)$ (error?)</td> </tr> <tr> <td>WC-C, Stoc</td> <td> </td> <td>?</td> <td>?</td> <td>[BB20, Thm 3.13] $O\left(\epsilon^{-8}\right)$ (single loop)</td> </tr> </tbody> </table> <p><strong>Remark:</strong></p> <ol> <li>content</li> </ol> <hr> <h2 id="to-have-a-better-bound-new-trends">To Have a Better Bound (New Trends)</h2> <p>The section above summarize the upper and lower bounds of the oracle complexity for finding an $\epsilon$-optimal solution or $\epsilon$-stationary points for minimization and minimax problems. Clearly this is not the end of the story, there are more and more optimization problems arising from various applications like machine learning and operation research<d-cite key="bottou2018optimization"></d-cite>, which come with more involved problem structure and complicated landscape characteristics; also sometimes we find it harder to explain algorithm behavior in practice using existing theory, e.g., <d-cite key="defazio2019ineffectiveness"></d-cite> shows that variance reduction may be ineffective on accelerating the training of deep learning models, which contrast the classical convergence theory. Here we discuss what could be potential interesting next steps.</p> <ul> <li>Richer Problem Structure</li> </ul> <p>In this notes, we only discussed minimization and minimax problems, while there are also many other important optimization problems with different structure, for example:</p> <ul> <li>Bilevel Optimization</li> </ul> \[\min_{x \in \mathcal{X}} \Phi(x) = F(x, y^*(x)) \quad \text{where} \quad y^*(x) = \underset{y \in \mathcal{Y}}{\arg\min} \, G(x, y),\] <p>Bilevel optimization covers minimax optimization as a special case. Over the past seven years, bilevel optimization has become increasingly popular due to its applications in machine learning. In particular, researchers have revisited first-order methods for solving (stochastic) bilevel optimization problems. Starting from [Reference A], which investigates double-loop methods for solving bilevel optimization, [Reference B] initiated the development of single-loop, single-timescale methods for stochastic bilevel optimization. This line of research leads to a simple single-timescale algorithm [Reference C] and multiple variance reduction techniques to achieve single-loop [Reference D]. Subsequent developments have focused on avoiding computing Hessian inverse of the lower-level problem [Reference E], developing fully first-order methods for solving bilevel optimization [Reference F], achieving global optimality [Reference G], addressing contextual/multiple lower-level problems [Reference H], handling constrained lower-level problems [Reference I], and bilevel reinforcement learning [Reference J] for model design and reinforcement learning with human feedback.</p> <p>Several questions remain open and are interesting to investigate:</p> <ol> <li>How to handle general lower-level problems with coupling constraints.</li> <li>How to accelerate fully first-order methods to match the optimal complexity bounds.</li> <li>How to establish non-asymptotic convergence guarantees for bilevel problems with convex lower levels.</li> </ol> <p>Also there appeared several other optimization problems with different formulations arising from practice, e.g.,</p> <ul> <li>Compositional Stochastic Optimization<d-cite key="wang2017stochastic"></d-cite> </li> </ul> \[\min_{x \in \mathcal{X}} F(x) = f(g(x)),\] <ul> <li>Performative Prediction (or Decision-Dependent Stochastic Optimization)<d-cite key="perdomo2020performative"></d-cite><d-cite key="drusvyatskiy2023stochastic"></d-cite> </li> </ul> \[\min_{x\in\mathcal{X}}\ f(x)\triangleq\mathbb{E}_{\xi\sim\mathcal{D}(x)}[f(x;\xi)]\] <ul> <li>Contextual Stochastic Optimization<d-cite key="bertsimas2020predictive"></d-cite><d-cite key="sadana2024survey"></d-cite> </li> </ul> \[\min_{x\in\mathcal{X}}\ f(x;z)\triangleq\mathbb{E}_{\xi\sim\mathcal{D}}[f(x;\xi)~|~Z=z]\] <ul> <li> <p>Landscape Analysis</p> <p>Since most deep learning problems are nonconvex, a vast amount of literature focus on finding a (generalized) stationary point of the original optimization problem, but the practice often showed that one could find global optimality for various structured nonconvex problems efficiently. In fact there is a line of research tailored for the global landscape of neural network training<d-cite key="sun2020global"></d-cite>, e.g., the interpolation condition holds for some overparameterized neural networks.</p> <p>Regarding such mismatch between theory and practice, one reason may be the coarse assumptions we applied in the theoretical analysis which cannot effectively characterize the landscape of objectives. Here we briefly summarize a few structures arising in recent works, which try to mix the gap between practice and theory:</p> <ul> <li>With numerical experiments disclosing uncommon structure in functions, some works proposed new assumptions which drive the algorithm design and corresponding theoretical analysis, which in turn reveals acceleration in empirical findings. For example, <d-cite key="zhang2019gradient"></d-cite> introduced the <em>relaxed smoothness assumption</em> (or $(L_0, L_1)$-smoothness) inspired by empirical observations on deep neural networks, and proposed a clipping-based first-order algorithm which enjoys both theoretical and practical outperformance.</li> <li>Another noteworthy work is <d-cite key="zhang2020adaptive"></d-cite>, which found <em>heavy-tailed noise</em> in stochastic gradients in neural network training practices, such evidence drove them to revise SGD and incorporate strategies like clipping in the algorithm design, which also outperformed in numerical experiments. The above two works, along with their more practical assumptions, inspired many follow-up works, evidented by their high citations according to Google Scholar<d-cite key="zhang2019citation"></d-cite><d-cite key="zhang2020citation"></d-cite>.</li> <li> <em>Hidden convexity</em> says that the original nonconvex optimization problem might admit a convex reformulation via a variable change. It appears in operations research [Reference], reinforcement learning [reference], control [reference]. Despite that the concrete transformation function is unknown, one could still solve the problem to global optimality efficiently.</li> <li>Another stream considers <em>Polyak-Łojasiewicz</em> (PL) or <em>Kurdyka-Łojasiewicz</em> (KL) type of conditions [Reference]. Such conditions imply that the (generalized) gradient norm could upper bound the optimality gap, implying that any (generalized) stationary point are also global optimal. However, establishing hidden convexity, PL, or KL condition is usually done in a case-by-case manner and could be challenging. See [Reference] for some examples.</li> </ul> </li> <li> <p>Beyond oracle model also regarding the oracle complexity model, because it mainly focuses on hard instances in the function class which may be far from practical instances, possibly the derived complexities may be a bit too conservative and they may not match the practice well, as the figure below illustrated.</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-28-opt-summary/practice_gap-480.webp 480w,/assets/img/2024-04-28-opt-summary/practice_gap-800.webp 800w,/assets/img/2024-04-28-opt-summary/practice_gap-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2024-04-28-opt-summary/practice_gap.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Gap Between General Worst-Case Complexity and Instance-Level Complexity Analysis (adapted from <d-cite key="zhang2022beyond"></d-cite>) </div> <ul> <li>Average complexity</li> <li>Time complexity</li> <li>large stepsize</li> <li>Communication complexity</li> </ul> <hr> <h2 id="conclusion">Conclusion</h2> <p>content</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-04-28-opt-summary.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"OPT-Eng-Ana/OPT-Eng-Ana.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 You R. Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"Edit the `_data/repositories.yml` and change the `github_users` and `github_repos` lists to include your own GitHub profile and repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-teaching",title:"teaching",description:"Materials for courses you taught. Replace this text with your description.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"nav-people",title:"people",description:"members of the lab or group",section:"Navigation",handler:()=>{window.location.href="/people/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"post-timer-iclr-sample-blog-post",title:"Timer ICLR Sample Blog Post",description:"Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.",section:"Posts",handler:()=>{window.location.href="/blog/2024/timer-example/"}},{id:"post-iclr-sample-blog-post",title:"ICLR Sample Blog Post",description:"Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.",section:"Posts",handler:()=>{window.location.href="/blog/2024/distill-example/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-think-twice-before-claiming-your-optimization-algorithm-outperformance-review-and-beyond",title:"Think Twice Before Claiming Your Optimization Algorithm Outperformance - Review and Beyond",description:"In this blog, we revisit the convergence analysis of first-order algorithms in minimization and minimax optimization problems. Within the classical oracle model framework, we review the state-of-the-art upper and lower bound results in various settings, aiming to identify gaps in existing research. With the rapid development of applications like machine learning and operation research, we further identify some recent works that revised the classical settings of optimization algorithms study.",section:"Posts",handler:()=>{window.location.href="/blog/2024/opt-summary/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2018/distill/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%79%6F%75@%65%78%61%6D%70%6C%65.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>